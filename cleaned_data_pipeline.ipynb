{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7a6nDx3sV19"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "!pip install isodate\n",
        "# Use the file ID to download\n",
        "!gdown --id 1np_zI9ll5MX73YBIpYwTHCGYCBl5MqNV\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "zip_path = \"/content/dataset.zip\"\n",
        "extract_path = \"/content/dataset\"\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extracted to:\", extract_path)"
      ],
      "metadata": {
        "id": "GOMpa_6AstSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "videos= pd.read_csv(\"/content/dataset/videos.csv\",parse_dates=[\"publishedAt\"])"
      ],
      "metadata": {
        "id": "J1zL5IBcs1Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from kneed import KneeLocator\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def cluster_videos_by_title(videos):\n",
        "    \"\"\"\n",
        "    Clusters video titles using embeddings + KMeans (optimal k via elbow method).\n",
        "\n",
        "    Args:\n",
        "        videos (pd.DataFrame): DataFrame containing at least a 'title' column.\n",
        "\n",
        "    Returns:\n",
        "        videos (pd.DataFrame): Original DataFrame with new 'cluster_label' column.\n",
        "        videos_cluster_summary (pd.DataFrame): Summary DataFrame with cluster stats.\n",
        "    \"\"\"\n",
        "    # --- 1. Preprocess titles (remove hashtags) ---\n",
        "    videos[\"clean_title\"] = videos[\"title\"].astype(str).apply(lambda x: re.sub(r\"#\", \"\", x).strip())\n",
        "\n",
        "    # --- 2. Embed titles ---\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    embeddings = model.encode(videos[\"clean_title\"].tolist(), show_progress_bar=True, batch_size=64)\n",
        "\n",
        "    # --- 3. Standardize embeddings (helps clustering stability) ---\n",
        "    scaler = StandardScaler()\n",
        "    embeddings_scaled = scaler.fit_transform(embeddings)\n",
        "\n",
        "    # --- 4. Find optimal k using elbow method ---\n",
        "    inertias = []\n",
        "    K_range = range(2, 15)  # Test between 2â€“15 clusters\n",
        "\n",
        "    for k in tqdm(K_range, desc=\"Finding optimal k\"):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        kmeans.fit(embeddings_scaled)\n",
        "        inertias.append(kmeans.inertia_)\n",
        "\n",
        "    # Use KneeLocator to find \"elbow\"\n",
        "    kl = KneeLocator(K_range, inertias, curve=\"convex\", direction=\"decreasing\")\n",
        "    best_k = kl.knee #if kl.knee else 5  # fallback if KneeLocator fails\n",
        "\n",
        "    print(f\"Optimal number of clusters (k): {best_k}\")\n",
        "\n",
        "    # --- 5. Final clustering with best_k ---\n",
        "    final_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
        "    videos[\"cluster_label\"] = final_kmeans.fit_predict(embeddings_scaled)\n",
        "\n",
        "    # --- 6. Create summary DataFrame ---\n",
        "    videos_cluster_summary = (\n",
        "        videos.groupby(\"cluster_label\")\n",
        "        .agg(\n",
        "            num_videos=(\"title\", \"count\"),\n",
        "            video_list=(\"title\", lambda x: list(x))\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    return videos, videos_cluster_summary\n",
        "\n",
        "videos, videos_cluster_summary = cluster_videos_by_title(videos)\n",
        "\n",
        "videos\n"
      ],
      "metadata": {
        "id": "tplwqg_rtI-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Define the mapping from cluster label to cluster name ---\n",
        "# Based on our analysis of the video titles in each cluster.\n",
        "cluster_name_map = {\n",
        "    0: 'Educational Skincare & Wellness',\n",
        "    1: 'Viral, Entertainment & Pop Culture',\n",
        "    2: 'Hair Styling & Transformations',\n",
        "    3: 'General Beauty for the Shorts Feed',\n",
        "    4: 'Makeup Tutorials & Challenges'\n",
        "}\n",
        "\n",
        "# --- 2. Create the new 'cluster_name' column ---\n",
        "# The .map() function will look at each value in 'cluster_label' and\n",
        "# replace it with the corresponding value from our dictionary.\n",
        "videos['cluster_name'] = videos['cluster_label'].map(cluster_name_map)\n",
        "\n",
        "\n",
        "# --- 3. (Optional) Verify the result ---\n",
        "# This will show you the first few rows with the new column\n",
        "print(\"DataFrame with the new 'cluster_name' column:\")\n",
        "print(videos[['title', 'cluster_label', 'cluster_name']].head())\n",
        "\n",
        "print(\"\\n--------------------------------------------------\\n\")\n",
        "\n",
        "# This will show you the counts for each new cluster name, confirming the mapping worked.\n",
        "print(\"Value counts for the new cluster names:\")\n",
        "videos"
      ],
      "metadata": {
        "id": "dT9i4k1Lt8Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "videos = videos[videos[\"cluster_name\"]!=\"Viral, Entertainment & Pop Culture\"]\n",
        "videos = videos.reset_index(drop=True)\n",
        "\n",
        "mask = videos[\"cluster_name\"] == \"Educational Skincare & Wellness\"\n",
        "videos.loc[mask] = (\n",
        "    videos[mask]\n",
        "    .drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
        ")\n",
        "\n",
        "videos = videos.dropna(subset=[\"cluster_name\"]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "WTwMyn7QvONx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "videos[\"cluster_name\"].unique()"
      ],
      "metadata": {
        "id": "Zgfh04e3vTf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from kneed import KneeLocator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "def add_subclusters(videos: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds a 'subcluster' column to the videos dataframe.\n",
        "    Each subcluster is named as '{cluster_name}_{subcluster_num}'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Make a copy so we don't modify in place\n",
        "    videos = videos.copy()\n",
        "    videos[\"subcluster\"] = None\n",
        "\n",
        "    # Mapping: cluster_name -> model\n",
        "    model_map = {\n",
        "        \"Hair Styling & Transformations\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"Makeup Tutorials & Challenges\": \"BAAI/bge-base-en-v1.5\",\n",
        "        \"Educational Skincare & Wellness\": \"intfloat/e5-base-v2\",\n",
        "        \"General Beauty for the Shorts Feed\": \"intfloat/e5-base-v2\",\n",
        "    }\n",
        "\n",
        "    for cluster_name in videos[\"cluster_name\"].dropna().unique():\n",
        "        cluster_data = videos[videos[\"cluster_name\"] == cluster_name].copy()\n",
        "\n",
        "\n",
        "        # Pick the right model\n",
        "        model_name = model_map.get(cluster_name, \"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        model = SentenceTransformer(model_name)\n",
        "\n",
        "        # Embed\n",
        "        embeddings = model.encode(cluster_data[\"title\"].tolist(), show_progress_bar=True)\n",
        "\n",
        "        # --- Elbow method to find optimal k ---\n",
        "        inertia = []\n",
        "        K = range(2, min(15, len(cluster_data)))\n",
        "        for k in K:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "            kmeans.fit(embeddings)\n",
        "            inertia.append(kmeans.inertia_)\n",
        "\n",
        "        kn = KneeLocator(K, inertia, curve=\"convex\", direction=\"decreasing\")\n",
        "        optimal_k = kn.knee or 3\n",
        "\n",
        "        # --- Final KMeans ---\n",
        "        final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "        cluster_labels = final_kmeans.fit_predict(embeddings)\n",
        "\n",
        "        # Assign back into main dataframe\n",
        "        subcluster_labels = [f\"{cluster_name}_{i}\" for i in cluster_labels]\n",
        "        videos.loc[cluster_data.index, \"subcluster\"] = subcluster_labels\n",
        "\n",
        "        print(f\"ðŸ“Š Cluster '{cluster_name}' split into {optimal_k} subclusters\")\n",
        "\n",
        "    return videos\n",
        "\n",
        "videos = add_subclusters(videos)\n",
        "videos"
      ],
      "metadata": {
        "id": "DGhjLdrDv3hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm  # <-- add tqdm\n",
        "\n",
        "def reassign_general_beauty(videos: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reassign misclassified 'General Beauty for the Shorts Feed' subclusters\n",
        "    into correct parent clusters and subclusters.\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer(\"intfloat/e5-base-v2\")\n",
        "\n",
        "    # mapping of misclustered subclusters -> target parent cluster\n",
        "    mapping = {\n",
        "        \"General Beauty for the Shorts Feed_1\": \"Makeup Tutorials & Challenges\",\n",
        "        \"General Beauty for the Shorts Feed_4\": \"Makeup Tutorials & Challenges\",\n",
        "        \"General Beauty for the Shorts Feed_2\": \"Hair Styling & Transformations\",\n",
        "        \"General Beauty for the Shorts Feed_5\": \"Educational Skincare & Wellness\"\n",
        "    }\n",
        "\n",
        "    # loop over each mapping\n",
        "    for bad_sub, correct_cluster in mapping.items():\n",
        "        target_rows = videos[videos[\"subcluster\"] == bad_sub]\n",
        "        if target_rows.empty:\n",
        "            continue\n",
        "\n",
        "        # reference rows from the correct cluster\n",
        "        ref_rows = videos[videos[\"cluster_name\"] == correct_cluster]\n",
        "\n",
        "        # embed titles\n",
        "        ref_embeddings = model.encode(ref_rows[\"title\"].tolist(), convert_to_tensor=True)\n",
        "        target_embeddings = model.encode(target_rows[\"title\"].tolist(), convert_to_tensor=True)\n",
        "\n",
        "        # assign based on cosine similarity (with tqdm progress bar)\n",
        "        for i, (idx, row) in enumerate(tqdm(target_rows.iterrows(),\n",
        "                                            total=len(target_rows),\n",
        "                                            desc=f\"Reassigning {bad_sub} â†’ {correct_cluster}\")):\n",
        "            sims = util.cos_sim(target_embeddings[i], ref_embeddings)[0].cpu().numpy()\n",
        "            best_match_idx = np.argmax(sims)\n",
        "            best_subcluster = ref_rows.iloc[best_match_idx][\"subcluster\"]\n",
        "\n",
        "            # update dataframe\n",
        "            videos.at[idx, \"subcluster\"] = best_subcluster\n",
        "            videos.at[idx, \"cluster_name\"] = correct_cluster\n",
        "\n",
        "    return videos\n",
        "\n",
        "videos = reassign_general_beauty(videos)\n",
        "videos"
      ],
      "metadata": {
        "id": "FTkPAoQ1wYeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_subclusters = [\n",
        "    \"Hair Styling & Transformations_0\",\"Hair Styling & Transformations_1\",\"Hair Styling & Transformations_3\",\n",
        "    \"Hair Styling & Transformations_4\",\"Hair Styling & Transformations_6\",\n",
        "    \"Makeup Tutorials & Challenges_1\",\"Makeup Tutorials & Challenges_2\",\"Makeup Tutorials & Challenges_3\",\n",
        "    \"Makeup Tutorials & Challenges_4\",\n",
        "    \"Educational Skincare & Wellness_0\",\"Educational Skincare & Wellness_1\",\"Educational Skincare & Wellness_2\"\n",
        "]\n",
        "\n",
        "videos[\"publishedAt\"] = pd.to_datetime(videos[\"publishedAt\"])\n",
        "videos = videos[videos[\"publishedAt\"] < \"2025-07-01\"]\n",
        "# filter dataframe\n",
        "videos = videos[videos[\"subcluster\"].isin(good_subclusters)].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "TQwZTxUwyF_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_videos_by_cluster(videos: pd.DataFrame, cluster_name: str, top_n: int = 100):\n",
        "    \"\"\"\n",
        "    For a given cluster_name, returns the top N videos (by view_velocity)\n",
        "    for each subcluster inside that cluster.\n",
        "    \"\"\"\n",
        "    # Filter videos for this cluster only\n",
        "    cluster_data = videos[videos[\"cluster_name\"] == cluster_name].copy()\n",
        "\n",
        "    if cluster_data.empty:\n",
        "        print(f\"âš ï¸ No videos found for cluster '{cluster_name}'\")\n",
        "        return {}\n",
        "\n",
        "    results = {}\n",
        "    for sub in sorted(cluster_data[\"subcluster\"].unique()):\n",
        "        sub_data = cluster_data[cluster_data[\"subcluster\"] == sub].copy()\n",
        "\n",
        "        # Sort by view_velocity (descending)\n",
        "        top_videos = (\n",
        "            sub_data.sort_values(\"view_velocity\", ascending=False)\n",
        "            .head(top_n)[[\"videoId\", \"clean_title\", \"view_velocity\", \"subcluster\"]]\n",
        "        )\n",
        "\n",
        "        print(f\"\\nðŸ”¥ Top {top_n} Videos in {sub}:\")\n",
        "        print(top_videos.to_string(index=False))\n",
        "\n",
        "        results[sub] = top_videos\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "'''\n",
        "array(['Educational Skincare & Wellness', 'Makeup Tutorials & Challenges',\n",
        "       'General Beauty for the Shorts Feed', nan,\n",
        "       'Hair Styling & Transformations'], dtype=object)\n",
        "'''\n",
        "\n",
        "# Assuming you already ran add_subclusters(videos)\n",
        "top_videos = top_videos_by_cluster(videos, \"Educational Skincare & Wellness\", top_n=100)\n",
        "\n"
      ],
      "metadata": {
        "id": "5-cQB9lZySNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "import itertools\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "def calculate_mape(y_true, y_pred):\n",
        "    \"\"\"Robust MAPE calculation that ignores zero actuals.\"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    non_zero_mask = y_true != 0\n",
        "    if not np.any(non_zero_mask):\n",
        "        return 0.0 if np.all(y_pred == 0) else np.inf\n",
        "    percentage_error = np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])\n",
        "    return np.mean(percentage_error) * 100\n",
        "\n",
        "def handle_anomalies(series, trend, seasonal, seasonal_periods, sigma=3.0):\n",
        "    \"\"\"\n",
        "    Detects and corrects anomalies in a time series based on Holt-Winters.\n",
        "\n",
        "    Anomalies are points that fall outside a confidence band defined by the\n",
        "    model's fitted values plus/minus a multiple (sigma) of the residual standard deviation.\n",
        "    \"\"\"\n",
        "    # Fit a model to the series to establish a baseline\n",
        "    model = ExponentialSmoothing(\n",
        "        series,\n",
        "        trend=trend,\n",
        "        seasonal=seasonal,\n",
        "        seasonal_periods=seasonal_periods\n",
        "    ).fit(optimized=True)\n",
        "\n",
        "    # Get the in-sample forecast (fitted values)\n",
        "    fitted_values = model.fittedvalues\n",
        "\n",
        "    # Calculate residuals (errors)\n",
        "    residuals = series - fitted_values\n",
        "\n",
        "    # Calculate the confidence interval bounds\n",
        "    residual_std = np.std(residuals)\n",
        "    upper_bound = fitted_values + sigma * residual_std\n",
        "    lower_bound = fitted_values - sigma * residual_std\n",
        "\n",
        "    # Identify anomalies\n",
        "    anomalies_upper = series > upper_bound\n",
        "    anomalies_lower = series < lower_bound\n",
        "\n",
        "    # Correct anomalies by replacing them with the boundary values\n",
        "    corrected_series = series.copy().astype(float)\n",
        "    corrected_series[anomalies_upper] = upper_bound[anomalies_upper]\n",
        "    corrected_series[anomalies_lower] = lower_bound[anomalies_lower]\n",
        "\n",
        "    return corrected_series\n",
        "\n",
        "\n",
        "def holt_winters_rollingcv(videos, subcluster, min_train=\"2020-01\", seasonal_range=(2, 31), forecast_horizon=6, anomaly_sigma=3.0):\n",
        "    \"\"\"\n",
        "    Holt-Winters with anomaly handling and rolling cross-validation for multi-step forecasting.\n",
        "\n",
        "    Anomaly Handling:\n",
        "      - Before training each CV fold, anomalies in the training data are detected and corrected.\n",
        "      - The seasonal parameter for anomaly detection is the SAME as the one being tested for forecasting.\n",
        "\n",
        "    Parameters:\n",
        "      - trend: ['add', 'mul']\n",
        "      - seasonal: ['add', 'mul']\n",
        "      - seasonal_periods: integers in seasonal_range\n",
        "      - forecast_horizon: how many months to forecast at each step (default=6)\n",
        "      - anomaly_sigma: The sensitivity for anomaly detection (default=3.0, i.e., 3 standard deviations).\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Data Preparation ---\n",
        "    subcluster_data = videos[videos[\"subcluster\"] == subcluster].copy()\n",
        "    subcluster_data[\"publishedAt\"] = pd.to_datetime(subcluster_data[\"publishedAt\"])\n",
        "\n",
        "    monthly_counts = (\n",
        "        subcluster_data\n",
        "        .groupby(pd.Grouper(key=\"publishedAt\", freq=\"M\"))[\"videoId\"]\n",
        "        .count()\n",
        "        .reset_index()\n",
        "        .rename(columns={\"videoId\": \"video_count\"})\n",
        "    )\n",
        "\n",
        "    if monthly_counts.empty:\n",
        "        return None\n",
        "\n",
        "    monthly_counts = monthly_counts.set_index(\"publishedAt\").asfreq(\"M\", fill_value=0)\n",
        "    monthly_counts = monthly_counts.loc[min_train:]\n",
        "\n",
        "    if len(monthly_counts) < seasonal_range[1] * 2:\n",
        "        print(f\"Not enough data for subcluster {subcluster}. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    # --- 2. Parameter grid ---\n",
        "    param_grid = {\n",
        "        'trend': ['add', 'mul'],\n",
        "        'seasonal': ['add', 'mul'],\n",
        "        'seasonal_periods': list(range(seasonal_range[0], seasonal_range[1] + 1))\n",
        "    }\n",
        "    all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # --- 3. Rolling CV loop ---\n",
        "    for params in all_params:\n",
        "        mape_scores = []\n",
        "\n",
        "        if len(monthly_counts) < params['seasonal_periods'] * 2:\n",
        "            continue\n",
        "\n",
        "        # rolling steps â€” forecast horizon must fit\n",
        "        for t in range(params['seasonal_periods'] * 2, len(monthly_counts) - forecast_horizon + 1):\n",
        "            train = monthly_counts.iloc[:t]\n",
        "            test = monthly_counts.iloc[t:t+forecast_horizon]\n",
        "\n",
        "            try:\n",
        "                # --- ANOMALY HANDLING STEP ---\n",
        "                # Clean the training data before fitting the final model\n",
        "                cleaned_train_series = handle_anomalies(\n",
        "                    train[\"video_count\"],\n",
        "                    trend=params['trend'],\n",
        "                    seasonal=params['seasonal'],\n",
        "                    seasonal_periods=params['seasonal_periods'],\n",
        "                    sigma=anomaly_sigma\n",
        "                )\n",
        "\n",
        "                # Fit the model on the CLEANED data\n",
        "                model = ExponentialSmoothing(\n",
        "                    cleaned_train_series, # Use the cleaned series here\n",
        "                    trend=params['trend'],\n",
        "                    seasonal=params['seasonal'],\n",
        "                    seasonal_periods=params['seasonal_periods']\n",
        "                ).fit(optimized=True)\n",
        "\n",
        "                # Forecast and evaluate against the original, untouched test data\n",
        "                forecast = model.forecast(forecast_horizon)\n",
        "                mape = calculate_mape(test[\"video_count\"], forecast)\n",
        "                mape_scores.append(mape)\n",
        "\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        if mape_scores:\n",
        "            avg_mape = np.mean(mape_scores)\n",
        "            results.append({\n",
        "                \"subcluster\": subcluster,\n",
        "                \"trend\": params['trend'],\n",
        "                \"seasonal\": params['seasonal'],\n",
        "                \"seasonal_periods\": params['seasonal_periods'],\n",
        "                \"avg_mape\": avg_mape\n",
        "            })\n",
        "\n",
        "    if not results:\n",
        "        return None\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "# --- EXAMPLE USAGE ---\n",
        "# Assuming 'videos' DataFrame is loaded\n",
        "results_list = []\n",
        "\n",
        "print(\"Starting rolling CV with ANOMALY HANDLING for all subclusters (6-month horizon)...\")\n",
        "for i in videos[\"subcluster\"].dropna().unique():\n",
        "    print(f\"\\n--- Processing Subcluster: {i} ---\")\n",
        "    # You can adjust anomaly_sigma here if needed, e.g., anomaly_sigma=2.5 for higher sensitivity\n",
        "    df_results = holt_winters_rollingcv(videos, i, seasonal_range=(2, 31), forecast_horizon=6, anomaly_sigma=3.0)\n",
        "    if df_results is not None:\n",
        "        best_row = df_results.sort_values(\"avg_mape\").iloc[0]\n",
        "        results_list.append(best_row)\n",
        "        print(f\"âœ… Best 6M MAPE {best_row['avg_mape']:.2f}% with params \"\n",
        "              f\"trend={best_row['trend']}, seasonal={best_row['seasonal']}, \"\n",
        "              f\"period={best_row['seasonal_periods']}\")\n",
        "    else:\n",
        "        print(f\"âŒ Skipped {i} (not enough data or model failure)\")\n",
        "\n",
        "forecasting_summary_df = pd.DataFrame(results_list).reset_index(drop=True)\n",
        "print(\"\\n\\n--- Forecasting Summary with Anomaly Handling (Best per Subcluster, 6-month horizon) ---\")\n",
        "if not forecasting_summary_df.empty:\n",
        "    display(forecasting_summary_df.sort_values(by=\"avg_mape\"))\n",
        "else:\n",
        "    print(\"No models could be successfully trained.\")"
      ],
      "metadata": {
        "id": "jINgKxij1xct"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}